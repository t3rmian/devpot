---
title: Niestabilne testy w procesie CI/CD
url: analiza-testÃ³w-niedeterministycznych
id: 18
tags:
  - java
  - android
  - testy
author: Damian Terlecki
date: 2019-12-08T20:00:00
---

IdÄ…c w gÃ³rÄ™ hierarchii testÃ³w, czÄ™sto napotykamy, na problem testÃ³w niestabilnych (ang. flaky tests). OkreÅ›lenie _flaky_, popularne w literaturze angielskiej oznacza, sytuacjÄ™, w ktÃ³rej test tej samej czÄ™Å›ci kodu zwraca rÃ³Å¼ne rezultaty (czasami koÅ„czy siÄ™ niepowodzeniem, mimo braku zmian w kodzie). Ze wzglÄ™du na to, Å¼e testy na wyÅ¼szym poziomie sÄ… na ogÃ³Å‚ wiÄ™ksze, wymagajÄ… wiÄ™cej zasobÃ³w i sprawdzajÄ… integracjÄ™ z wieloma komponentami, to wÅ‚aÅ›nie tej kategorii testÃ³w najczÄ™Å›ciej dotyczy problem niestabilnoÅ›ci. Testy te mogÄ… obejmowaÄ‡ pewnÄ… komunikacjÄ™ sieciowÄ…, mogÄ… Å‚adowaÄ‡ duÅ¼e dane, czÄ™Å›Ä‡ z nich moÅ¼e dziaÅ‚aÄ‡ w tle, a kolejnoÅ›Ä‡ synchronizacji moÅ¼e byÄ‡ nie zawsze deterministyczna. W innych przypadkach wskazujÄ… one na problemy z wydajnoÅ›ciÄ…, bÄ…dÅº z konfiguracjÄ… Å›rodowiska, ostatecznie, mogÄ… po prostu sprowadzaÄ‡ siÄ™ do niepoprawnych zaÅ‚oÅ¼eÅ„ podczas implementacji testu.

## Statystyka

Wraz ze wzrostem liczby testÃ³w integracyjnych, UI, sieciowych i wspÃ³Å‚bieÅ¼noÅ›ci wzrastajÄ… szanse na niepowodzenie integracyjnego procesu budowania (CI). WyobraÅº sobie, Å¼e 10% twoich testÃ³w charakteryzuje siÄ™ niestabilnoÅ›ciÄ…, np.: kaÅ¼dy z nich koÅ„czy siÄ™ niepowodzeniem raz na 1000 przebiegÃ³w. 1 na 1000, czyli 0,1%! Nie brzmi to tak Åºle, prawda? Teraz wyobraÅº sobie, Å¼e mamy 1000 testÃ³w, nie za maÅ‚o, nie za duÅ¼o. Zatem dla 100 testÃ³w, ktÃ³re sÄ… w tym sensie niedeterministyczne, skumulowane prawdopodobieÅ„stwo niepowodzenia weryfikacji wyniesie:

<img src="/img/hq/flaky-tests-probability-failed-test.gif" alt="P(FAILED_TEST) = 1/1000" class="img-formula">
<img src="/img/hq/flaky-tests-probability-failed-build.gif" alt="P(SUCCESSFUL_TEST) = P(\Omega) - 1/1000 = 999/1000" class="img-formula">
<img src="/img/hq/flaky-tests-probability-successful-test.gif" alt="P(SUCCESSFUL_BUILD) = P(SUCCESSFUL_TEST_1) âˆ© P(SUCCESSFUL_TEST_2) âˆ© P(SUCCESSFUL_TEST_3)  âˆ©  ...  âˆ© P(SUCCESSFUL_TEST_N) = (999/1000)^100 â‰ˆ 90%" title="P(SUCCESSFUL_BUILD) = P(SUCCESSFUL_TEST_1) âˆ© P(SUCCESSFUL_TEST_2) âˆ© P(SUCCESSFUL_TEST_3)  âˆ©  ...  âˆ© P(SUCCESSFUL_TEST_N) = (999/1000)^100 â‰ˆ 90%" class="img-formula">
<img src="/img/hq/flaky-tests-probability-successful-build.gif" alt="P(FAILED_BUILD) = P(\Omega) - P(SUCCESSFUL_BUILD) = 10%" class="img-formula">

Ok, to zaczyna brzmieÄ‡ juÅ¼ jak **problem**. Statystycznie co dziesiÄ…ty proces zakoÅ„czy siÄ™ niepowodzeniem, pomimo praktycznie 100%-owej szansy na powodzenie kaÅ¼dego testu. Proces bÄ™dziemy musieli analizowaÄ‡, czÄ™sto dochodzÄ…c do wniosku, Å¼e zarÃ³wno test, jak i kod wyglÄ…dajÄ… poprawnie, a na rezultat miaÅ‚ wpÅ‚yw jakiÅ› czynnik zewnÄ™trzny. Jednak, aby zobaczyÄ‡ ogÃ³lny obraz prawdopodobieÅ„stwa niepowodzenia weryfikacji, warto przeanalizowaÄ‡ szerszy zakres parametrÃ³w:

<center>
<table>
<thead>
    <tr>
        <th class="corner-header">ğŸ ‡ Liczba testÃ³w \<br/>PrawdopodobieÅ„stwo niepowodzenia testu ğŸ †</th>
        <th>1 / 100 000</th>
        <th>1 / 10 000</th>
        <th>1 / 1 000</th>
        <th>1 / 100</th>
        <th>1 / 10</th>
    </tr>
</thead>
<tbody>
 <tr><td class="th">1</td><td>0%</td><td>0%</td><td>0%</td><td class="warn">1%</td><td class="err">10%</td></tr>
 <tr><td class="th">10</td><td>0%</td><td>0%</td><td class="warn">1%</td><td class="err">10%</td><td class="err">65%</td></tr>
 <tr><td class="th">50</td><td>0%</td><td>0%</td><td class="err">5%</td><td class="err">39%</td><td class="err">99%</td></tr>
 <tr><td class="th">100</td><td>0%</td><td class="warn">1%</td><td class="err">10%</td><td class="err">63%</td><td class="err">100%</td></tr>
 <tr><td class="th">250</td><td>0%</td><td class="warn">2%</td><td class="err">22%</td><td class="err">92%</td><td class="err">100%</td></tr>
 <tr><td class="th">500</td><td>0%</td><td class="err">5%</td><td class="err">39%</td><td class="err">99%</td><td class="err">100%</td></tr>
 <tr><td class="th">1000</td><td class="warn">1%</td><td class="err">10%</td><td class="err">63%</td><td class="err">100%</td><td class="err">100%</td></tr>
 <tr><td class="th">2000</td><td class="warn">2%</td><td class="err">18%</td><td class="err">86%</td><td class="err">100%</td><td class="err">100%</td></tr>
</tbody>
</table>
<b>Szansa niepowodzenia procesu weryfikacji w procesie CI/CD</b>
</center>

PrzeglÄ…dajÄ…c tabelkÄ™, z Å‚atwoÅ›ciÄ… odkryjemy sytuacje, w ktÃ³rych spÄ™dzimy wiÄ™cej czasu sprawdzajÄ…c, dlaczego, kompilacja siÄ™ nie powiodÅ‚a, niÅ¼ robiÄ…c coÅ› produktywnego. OczywiÅ›cie czasami moÅ¼emy zaadaptowaÄ‡ test do pewnych warunkÃ³w, ale w wielu przypadkach nie przewidzimy wszystkiego, a nasz wpÅ‚yw na samo Å›rodowisko moÅ¼e byÄ‡ minimalny. InnÄ… opcjÄ… jest usuniÄ™cie testu lub zignorowanie jego wynikÃ³w, jednakÅ¼e czÄ™sto stanowiÄ… one wartoÅ›Ä‡ dodanÄ… i dostarczajÄ… nam **dodatkowych informacji** na dziaÅ‚ania testowanych elementÃ³w.

Trzecim sposobem na rozwiÄ…zanie problemu jest powtarzanie testÃ³w niestabilnych. JeÅ›li mamy test, ktÃ³ry koÅ„czy siÄ™ niepowodzeniem raz na dziesiÄ™Ä‡ razy, powtarzajÄ…c go raz, powinniÅ›my obniÅ¼yÄ‡ prawdopodobieÅ„stwo niepowodzenia do 1/100; powtarzajÄ…c go dwa razy â€” do 1/1000. Przy bazowym prawdopodobieÅ„stwie wynoszÄ…cym 1/100 uzyskamy jeszcze wiÄ™kszy spadek. DziÄ™ki temu, w teorii, bezproblemowo przejdziemy od prawej krawÄ™dzi powyÅ¼szej tabeli (duÅ¼y wskaÅºnik awaryjnoÅ›ci) do lewej (bardzo niska szansa na niepowodzenie).

## Java i Android

PoniewaÅ¼ niestabilne testy sÄ… dosyÄ‡ czÄ™stym problemem podczas weryfikacji interfejsu uÅ¼ytkownika w Androidzie (podobnÄ… kategoriÄ… sÄ… testy Selenium), pokaÅ¼Ä™, jak zaimplementowaÄ‡ mechanizm powtÃ³rzeÅ„ testÃ³w na tej platformie. W przeszÅ‚oÅ›ci ta funkcja byÅ‚a standardowo dostÄ™pna wraz z adnotacjÄ… [@FlakyTest](https://developer.android.com/reference/android/test/FlakyTest.html). Wraz z wprowadzeniem pakietu testowego `androidx.test` opcja ta zostaÅ‚a niestety usuniÄ™ta. Niemniej jednak, jeÅ›li korzystamy z JUnita to nie mamy czym siÄ™ przejmowaÄ‡. JUnit jest doÅ›Ä‡ potÄ™Å¼nym narzÄ™dziem i zapewnia nam interfejs pozwalajÄ…cy zaimplementowaÄ‡ tÄ™ funkcjonalnoÅ›Ä‡ w kilku prostych krokach. W podobny sposÃ³b moÅ¼na to zrealizowaÄ‡ w standardowej Javie.

### RetryStatement

Zacznijmy od samego rdzenia. KaÅ¼da czÄ™Å›Ä‡ kodu klasy testowej opakowywana jest w `org.junit.runners.model.Statement` za pomocÄ… metody `evaluate`.
Pod uwagÄ™ barny jest nie tylko kod metody z adnotacjÄ… `@Test`, ale takÅ¼e kod pozostaÅ‚ych metod z adnotacjami, takimi jak `@BeforeClass` czy `@AfterClass`. W zwiÄ…zku z tym, pierwszym krokiem do zaimplementowania naszej funkcji ponawiania jest udekorowanie tej klasy w nastÄ™pujÄ…cy sposÃ³b:

```java
class RetryStatementDecorator extends Statement {

    private static final String TAG = RetryStatementDecorator.class.getSimpleName();

    private final int tryLimit;
    private final Statement base;
    private final Description description;

    RetryStatementDecorator(Statement base, Description description, int tryLimit) {
        this.base = base;
        this.description = description;
        this.tryLimit = tryLimit;
    }

    @Override
    public void evaluate() throws Throwable {
        Throwable caughtThrowable = null;

        for (int i = 0; i < tryLimit; i++) {
            try {
                base.evaluate();
                return;
            } catch (Throwable t) {
                caughtThrowable = t;
                Log.w(TAG, String.format(Locale.getDefault(), "%s: run %d failed", description.getDisplayName(), (i + 1)));
            }
        }
        Log.w(TAG, String.format(Locale.getDefault(), "%s: giving up after %d failures", description.getDisplayName(), tryLimit));
        //noinspection ConstantConditions
        throw caughtThrowable;
    }

}
```

### RetryTestRule

NastÄ™pnÄ… rzeczÄ…, ktÃ³rÄ… musimy zrobiÄ‡, jest zaaplikowanie naszej klasy do testÃ³w. Na pewno moÅ¼emy uÅ¼yÄ‡ interfejsu `TestRule` i zaimplementowaÄ‡ wÅ‚asny odpowiednik:

```java
public class RetryRule implements TestRule {

    private int tryLimit;

    public RetryRule(int tryLimit) {
        this.tryLimit = tryLimit;
    }

    public Statement apply(Statement base, Description description) {
        return new RetryStatementDecorator(base, description, tryLimit);
    }

}
```

Z bardzo prostym sposobem uÅ¼ycia w klasie testowej:
```java
    @Rule
    public RuleChain testRule = RuleChain
            .outerRule(new ActivityTestRule<>(MainActivity.class, true, true))
            .around(new ScreenshotOnTestFailedRule());
            .around(new RetryRule());
```

I to zadziaÅ‚a w wielu przypadkach, ogÃ³lnie jednak **tylko metoda z adnotacjÄ… @Test zostanie powtÃ³rzona**. Oznacza to, Å¼e dany test zostanie ponownie wykonany dla stanu *Activity*, jaki pozostaÅ‚ po nieudanym wykonaniu. WyobraÅº sobie, Å¼e masz test, ktÃ³ry przykÅ‚adowo otwiera menu i szuka w nim okreÅ›lonego elementu. Podczas kolejnej prÃ³by test zakoÅ„czy siÄ™ niepowodzeniem przy otwieraniu menu, poniewaÅ¼ menu bÄ™dzie juÅ¼ otwarte. MoÅ¼esz oczywiÅ›cie obejÅ›Ä‡ ten problem w ten czy inny sposÃ³b, ale najlepszym sposobem byÅ‚oby wdroÅ¼enie powtÃ³rzenia na wyÅ¼szym poziomie â€” na poziomie *runnera*.

### RetryRunner

DochodzÄ…c do tego momentu, caÅ‚a implementacja moÅ¼e wydawaÄ‡ siÄ™ nieco zÅ‚oÅ¼ona, w zasadzie jest jednak bardzo prosta. Chcemy zaimplementowaÄ‡ funkcjÄ™ ponawiania zarÃ³wno na poziomie bloku kodu odnoszÄ…cego siÄ™ do klasy (`@BeforeClass`), jak i na poziomie bloku metody (`@Test1`). W ten sposÃ³b za kaÅ¼dym razem, gdy nasz test siÄ™ nie powiedzie, bÄ™dziemy mieli pewnoÅ›Ä‡, Å¼e nasze *Activity* zostanie stworzone na nowo. W tym celu rozszerzymy `AndroidJUnit4ClassRunner`. Prawdopodobnie moglibyÅ›my uÅ¼yÄ‡ tutaj `BlockJUnit4ClassRunner`, poniewaÅ¼ klasa ta zawiera wszystko, czego potrzebujemy, jednak jeÅ›li zerkniesz na implementacjÄ™ `AndroidJUnit4`, czyli klasy, ktÃ³ra jest standardowo wykorzystywana do testÃ³w w Androidzie, zobaczysz, Å¼e inicjuje ona wÅ‚aÅ›nie `androidx.test.internal.runner.junit4.AndroidJUnit4ClassRunner`. DobrÄ… zasadÄ… jest ograniczanie zmian do minimum.

```java
public class RetryRunner extends AndroidJUnit4ClassRunner {

    public RetryRunner(Class<?> klass) throws InitializationError {
        super(klass);
    }

    @Override
    protected Statement classBlock(RunNotifier notifier) {
        return new RetryStatementDecorator(super.classBlock(notifier), getDescription(), BuildConfig.IT_TEST_TRY_LIMIT);
    }

    @Override
    protected Statement methodBlock(FrameworkMethod method) {
        return new RetryStatementDecorator(super.methodBlock(method), describeChild(method), BuildConfig.IT_TEST_TRY_LIMIT);
    }

}
```
CaÅ‚kiem proste, prawda? Ponownie wykorzystujemy zdefiniowanÄ… wczeÅ›niej klasÄ™ `RetryStatementDecorator`, dekorujÄ…c instrukcjÄ™ otrzymanÄ… z implementacji klas nadrzÄ™dnych. Do limitu liczby ponownych prÃ³b uÅ¼yÅ‚em niestandardowego pola generowanego podczas procesu budowy w Gradle'u wÅ‚aÅ›ciwego dla konfiguracji *debug*:

```gradle
android {
    buildTypes {
        debug {
            it.buildConfigField "int", "IT_TEST_TRY_LIMIT", ("true" == System.getenv("CI") ? 3 : 1).toString()
        }
    }
}
```
Wykorzystanie naszego nowego runnera polega na zamianie wartoÅ›ci adnotacji `@RunWith(AndroidJUnit4.class)` na `@RunWith(RetryRunner.class)`. W specyficznych przypadkach moÅ¼esz takÅ¼e wyprÃ³bowaÄ‡ opcjÄ™ z udokumentowanÄ… w javadocu klasy `AndroidJUnit4`:

> This implementation will delegate to the appropriate runner based on the build-system provided value. A custom runner can be provided by specifying the full class name in a 'android.junit.runner' system property.

JednakÅ¼e, jeÅ›li runner bÄ™dzie dziaÅ‚aÅ‚ na urzÄ…dzeniu/emulatorze (do czego nawiÄ…zujÄ™ w tym artykule), to miej na uwadze, Å¼e trudno tam ustawiÄ‡ wartoÅ›ci parametrÃ³w systemowych. Przydatna moÅ¼e byÄ‡ rÃ³wnieÅ¼ klasa `RunnerBuilder`, poniewaÅ¼ moÅ¼e byÄ‡ przekazana jako parametr do [runnera odpowiedzialnego za instrumentacjÄ™](https://developer.android.com/reference/android/support/test/runner/AndroidJUnitRunner).

Teraz dla testu zakoÅ„czonego niepowodzeniem, powinniÅ›my uzyskaÄ‡ uzyskaÄ‡ coÅ› takiego:
```java
2019-12-01 16:15:49.176 4818-4834/? W/RetryStatementDecorator: onAboutCreate(io.github.t3r1jj.pbmap.about.AboutActivityIT): run 1 failed
2019-12-01 16:15:51.788 4818-4834/? W/RetryStatementDecorator: onAboutCreate(io.github.t3r1jj.pbmap.about.AboutActivityIT): run 2 failed
2019-12-01 16:15:54.053 4818-4834/? W/RetryStatementDecorator: onAboutCreate(io.github.t3r1jj.pbmap.about.AboutActivityIT): run 3 failed
2019-12-01 16:15:54.054 4818-4834/? W/RetryStatementDecorator: onAboutCreate(io.github.t3r1jj.pbmap.about.AboutActivityIT): giving up after 3 failures
2019-12-01 16:15:54.056 4818-4834/? E/TestRunner: junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:48)
        at junit.framework.Assert.fail(Assert.java:56)
        at io.github.t3r1jj.pbmap.about.AboutActivityIT.onAboutCreate(AboutActivityIT.java:76)
        at java.lang.reflect.Method.invoke(Native Method)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at androidx.test.internal.runner.junit4.statement.RunAfters.evaluate(RunAfters.java:61)
        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
        at androidx.test.rule.ActivityTestRule$ActivityStatement.evaluate(ActivityTestRule.java:531)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at io.github.t3r1jj.pbmap.testing.RetryStatementDecorator.evaluate(RetryStatementDecorator.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at io.github.t3r1jj.pbmap.testing.RetryStatementDecorator.evaluate(RetryStatementDecorator.java:30)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.junit.runners.Suite.runChild(Suite.java:128)
        at org.junit.runners.Suite.runChild(Suite.java:27)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
        at androidx.test.internal.runner.TestExecutor.execute(TestExecutor.java:56)
        at androidx.test.runner.AndroidJUnitRunner.onStart(AndroidJUnitRunner.java:392)
        at android.app.Instrumentation$InstrumentationThread.run(Instrumentation.java:2074)
```

### Wtyczki Surefire i Failsafe

JeÅ›li uÅ¼ywasz wtyczek *Surefire* lub *Failsafe* w swoim projekcie, sprawa moÅ¼e byÄ‡ znacznie prostsza. Te dwie wtyczki zapewniajÄ… [interfejs](https://maven.apache.org/surefire/maven-surefire-plugin/examples/rerun-failing-tests.html), umoÅ¼liwiajÄ…cy ponowne uruchomienie nieudanych testÃ³w (JUnit 4.x):
```bash
mvn -Dsurefire.rerunFailingTestsCount=3 test
```

## Podsumowanie

Ponowne uruchamianie testÃ³w niestabilnych pozwala zmniejszyÄ‡ liczbÄ™ niepowodzeÅ„ w naszym procesie CI/CD, bez koniecznoÅ›ci usuwania problematycznych testÃ³w. MogÄ… one bowiem dostarczaÄ‡ uÅ¼ytecznych informacji, choÄ‡ ogÃ³lnie dobrym pomysÅ‚em jest przeanalizowanie kaÅ¼dego przypadku przed zastosowaniem takiej funkcjonalnoÅ›ci. JeÅ›li chcesz dowiedzieÄ‡ siÄ™ wiÄ™cej o testach niedeterministycznych, polecam posty [Johna Micco](https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html) i [Jeffa Listfielda](https://testing.googleblog.com/2017/04/where-do-our-flaky-tests-come-from.html) na temat niestabilnoÅ›ci testÃ³w na blogu Google.